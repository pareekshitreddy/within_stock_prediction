# -*- coding: utf-8 -*-
"""WITHIN_Assesment_Stock_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GEv5CWg3GZ74WgZACWVKTlMC_VPQiyve
"""

import pandas as pd
import numpy as np
import yfinance as yf
import requests
import nltk
import re
import json
import torch
import transformers
from bs4 import BeautifulSoup
from transformers import BertTokenizer, BertForSequenceClassification
from torch.nn.functional import softmax
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler
from ta import add_all_ta_features
from ta.utils import dropna
from sklearn.feature_selection import RFE

# Load pre-trained FinBERT model for sentiment analysis
tokenizer = BertTokenizer.from_pretrained("ProsusAI/finbert")
model = BertForSequenceClassification.from_pretrained("ProsusAI/finbert")

def get_stock_data(ticker, start, end):
    stock = yf.download(ticker, start=start, end=end)
    stock.reset_index(inplace=True)
    stock.columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']
    stock = dropna(stock)
    stock = add_all_ta_features(stock, open="Open", high="High", low="Low", close="Close", volume="Volume")
    return stock

# Fetch financial news headlines from an API
def get_financial_news():
    url = "https://newsapi.org/v2/everything?q=stock%20market&apiKey=your_api_key_here"
    response = requests.get(url)
    news_data = response.json()
    articles = [article['title'] for article in news_data['articles']]
    return articles

# Advanced Sentiment Analysis using FinBERT
def analyze_sentiment(news):
    sentiments = []
    for article in news:
        inputs = tokenizer(article, return_tensors="pt", truncation=True, padding=True)
        outputs = model(**inputs)
        scores = softmax(outputs.logits, dim=1).detach().numpy()[0]
        sentiment_score = scores[2] - scores[0]  # Positive - Negative
        sentiments.append(sentiment_score)
    return sentiments

# Feature Engineering
def create_lag_features(data, lag=5):
    for i in range(1, lag+1):
        data[f'Close_Lag_{i}'] = data['Close'].shift(i)
    data.dropna(inplace=True)
    return data

# Feature Selection Function
def select_features(X_train, y_train, X_val):
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rfe = RFE(estimator=rf, n_features_to_select=10)  # Selecting top 10 features
    X_train_selected = rfe.fit_transform(X_train, y_train.values.ravel())
    X_val_selected = rfe.transform(X_val)
    selected_features = X_train.columns[rfe.support_]
    print("Selected Features:", list(selected_features))
    return X_train_selected, X_val_selected, selected_features

# Main pipeline
def main():
    stock_data = get_stock_data('AAPL', '2023-01-01', '2024-01-01')
    news_articles = get_financial_news()
    sentiments = analyze_sentiment(news_articles)

    sentiment_df = pd.DataFrame({'Date': pd.date_range(start='2023-01-01', periods=len(sentiments), freq='ME'),
                                 'Sentiment': sentiments})
    stock_data['Date'] = pd.to_datetime(stock_data['Date'])
    sentiment_df['Date'] = pd.to_datetime(sentiment_df['Date'])
    merged_data = pd.merge(stock_data, sentiment_df, on='Date', how='left').fillna(0)
    merged_data = create_lag_features(merged_data)
    features = [col for col in merged_data.columns if col not in ['Date', 'Close']]
    X = merged_data[features]
    y = merged_data[['Close']]
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

    # Feature Selection
    X_train_selected, X_val_selected, selected_features = select_features(X_train, y_train, X_val)

    # Scaling
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    X_train_scaled = scaler_X.fit_transform(X_train_selected)
    X_val_scaled = scaler_X.transform(X_val_selected)
    X_test_selected = X_test[selected_features]
    X_test_scaled = scaler_X.transform(X_test_selected)
    y_train_scaled = scaler_y.fit_transform(y_train)
    y_val_scaled = scaler_y.transform(y_val)
    y_test_scaled = scaler_y.transform(y_test)

    # Reshape for LSTM
    X_train_scaled = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
    X_val_scaled = np.reshape(X_val_scaled, (X_val_scaled.shape[0], X_val_scaled.shape[1], 1))
    X_test_scaled = np.reshape(X_test_scaled, (X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

    # LSTM Model
    lstm_model = Sequential([
        LSTM(50, return_sequences=True, input_shape=(X_train_scaled.shape[1], 1)),
        Dropout(0.2),
        LSTM(50, return_sequences=False),
        Dropout(0.2),
        Dense(25),
        Dense(1)
    ])
    lstm_model.compile(optimizer='adam', loss='mean_squared_error')

    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    history = lstm_model.fit(X_train_scaled, y_train_scaled, validation_data=(X_val_scaled, y_val_scaled), epochs=50, batch_size=16, verbose=1, callbacks=[early_stopping])

    lstm_predictions = lstm_model.predict(X_test_scaled)
    lstm_predictions = scaler_y.inverse_transform(lstm_predictions)
    y_test_actual = scaler_y.inverse_transform(y_test_scaled)

    print(f'MAE: {mean_absolute_error(y_test_actual, lstm_predictions)}, MSE: {mean_squared_error(y_test_actual, lstm_predictions)}')

    plt.figure(figsize=(10,5))
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    plt.figure(figsize=(10,5))
    plt.plot(merged_data['Date'].iloc[-len(y_test):], y_test_actual, label='Actual')
    plt.plot(merged_data['Date'].iloc[-len(y_test):], lstm_predictions, label='Forecasted')
    plt.title('Stock Price Prediction')
    plt.xlabel('Date')
    plt.ylabel('Close Price')
    plt.legend()
    plt.show()
if __name__ == "__main__":
    main()

